# Bank Statement Analyser (banksa)
 
An app that extracts your transactions using LLMs and categorizes them to let you know how your expenses are going.

This app was initially made as the project for [LLM Zoomcamp 2024](https://github.com/DataTalksClub/llm-zoomcamp) of 2024 from [DataTalks.Club](https://datatalks.club/), to whom I owe my thanks.

## Content

1. [The problem](#the-problem)
2. [The solution](#the-solution)
3. [Architecture](#architecture)
4. [Setup](#setup)
    1. [With Docker Compose](#with-docker-compose)
    2. [Database schema](#database-schema)
5. [The UI features](#the-ui-features)
    1. [Upload](#upload)
    2. [Feedback](#feedback)
    3. [Transactions](#transactions)
    4. [Dashboards](#dashboards)
    5. [Evaluations](#evaluations)
    6. [Workflows](#workflows)
6. [LLMs used](#llms-used)
7. [UI Images](#ui-images)

## The problem

You probably have a PDF for each month of your Credit Card Statement or Account statement.
Those transactions may not be loaded and centralized for every bank where you have one of those products.

And you would like to have a centralized solution that takes all the transactions, "categorizes" them in order to show you all in different ways:
1. Simple list of all transactions
2. Some dashboards with all the categories and which is the category you use the most weighted by the sum of amounts. Also the category where you consume the least of your credit or expenses.

## The solution

With this app you would:
1. Upload the PDF and the content's text will be extracted
2. An LLM model would be used to extract the transactions from the PDF text.
3. All transactions will have a initial category assinged, so the user could give feedback to indicate if the category suits the description of the transaction. The feedback over a category can only be made once.
4. The workflows (pipelines in Mage) will also try to generate the right category after the transactions are extracted using your own feedback as reference. All this with the help of a RAG.

## Architecture

![arc](/docs/architecture_diagram.drawio.svg)

## Setup

> This setup has been used in a Github Codespaces with a 4-core machine.

### With Docker Compose

Execute these 2 commands to complete the creation of the environment:
```bash
# create all the images and pull some others needed
docker compose build
# run all the images
docker compose up -d
```

For more specifications of setup, please use the [setup](/docs/setup.md) guide.

### Database schema

As DB engine we are using **PostgreSQL** and for IDE **PgAdmin**.

For more details on how to get the database ready for this app, use the [Database setup](/docs/db_schema.md) guide.

## The UI features

This app allows the user to perform some functions in order to let them see all their expenses; upload bank statement files, see the extracted transactions, give feedback on categories generated by LLM upon transactions extraction, and give feedback on new categories assigned to transactions based on their description. It also allows users to see other functionalities working in the background like [evaluations](#evaluations).

### Upload

1. Letting the user upload PDF files. The files are supposed to be bank statement files containing transactions.
2. Each file's text is extracted and stored in the database.
3. Every file uploaded can only be loaded into the system once. The key fields to identify its uniqueness are the `file_name` and `file_size`.
4. After background process #1, the user can also see `transactions` from each file.
5. The user can also see some transaction descriptions to give `feedback` if there are any.

Files uploaded are shown at [Home](/docs/app/app_home.png).

### Feedback

1. In order to give `feedback`, the user can mark `Yes` or `No` if the first category (guessed by the LLM on extraction of `json` from the file text) is correct.
2. When answered `Yes`, the transactions with the same descriptions get updated.
3. When `No`, the user can give a suggestion on `category` and that `category` will be used in all existing transactions with the same description.
4. The user can only give one feedback on each transaction description.
5. Feedback made on each description would be considered as `knowledge base` for the time when the app would need to predict the category of unanswered transaction descriptions. See process #4.

[Evidence](/docs/app/app_feedback.png)

### Transactions

1. The user will have a section available in the `App` for transactions where they would see every transaction extracted from each file (filtered by file).
2. Each transaction has:
    1. **Date**: on which the transaction was made 
    2. **Description**: The whole description of the commerce or motive of the transaction
    3. **Category**: The initial category predicted from the transaction's description, or the predicted category if no feedback was made (using the LLM and the reference from the knowledge base created using feedbacks), or finally the suggested category from the feedback made if the initial was not correct.
    4. **Amount**: as self-described
    5. **Currency**: used in the transaction
    6. **Type**: Debit or credit (all payments are treated as `Credit` and all other transactions as `Debit`)

[Evidence](/docs/app/app_transactions.png)

### Dashboards

In the app, there are 2 types of graph pages based on the role of the user:

1. **User**: Can see all data related to the details of the transactions grouped by `amount` and `quantity`.
2. **Admin**: (_more like a health checker for the app_) Can see:
    1. The distribution of all feedbacks, how many descriptions were `downvoted` and how many were `upvoted`.
    2. The result of the `cosine similarity` evaluation over time.
    3. The result of the `hit-rate` evaluation over time.

[User Evidence](/docs/app/app_dashboards_user.png)

[Admin Evidence](/docs/app/app_dashboards_admin.png)

### Evaluations

For the purpose of this solution, we tried to evaluate the texts from `description` and `category` combined and separately, and found the result is better when the `description` is evaluated, as can be appreciated in the notebook [evaluate_vector_search_fields.ipynb](/notebooks/evaluate_vector_search_fields.ipynb).

We used 2 metrics for evaluating the results between the knowledge base used in the VectorDB (Elasticsearch) and the results:
1. **Hit-rate**: how relevant each text is when searched with itself.
2. **MRR (Mean Reciprocal Rank)**: How each text is related to each other text.

We used the data from the knowledge base and tried each metric on every field combination. We found the result is optimal when `description` is used, so we used it in the evaluation workflow.

[Evidence](/docs/app/app_dashboards_admin.png)

### Workflows

All these processes are made in a Mage Pipeline.

1. **fill_files_missing_json**: This process gets the files with no `json` data extracted (the `json` data contains the `transactions` and the main data from the statement like `date`).

    Also, these transactions' unique descriptions are stored in the knowledge database for further feedback from the user.

2. **index_knowledge_base**: Every 10 minutes, this process will look for every feedback made by the user and not stored in the vectorDB (ElasticSearch) to save them in Elasticsearch.

3. **extract_file_transactions**: Every 10 minutes, this process will look for the files whose transactions are not extracted and saved to the database. 

4. **complete_transactions_categories**: This process runs every 10 minutes and tries to predict the category of every not-confirmed transaction's category using LLM and the `knowledge_base` created by the user's feedback.

5. **get_evaluations**: Execute 2 evaluations of the effectiveness of the prediction and the feedback. The first is a _cosine similarity_ between suggested categories (on feedback) and the initial category made when extracting the transactions. The other evaluation used is _hit rate_ to determine how many of all transactions had an incorrect category, determined by the down votes on feedback results. For more information, look at the `Evaluations` in `The Operations` section.

**_Warning_**: In the Mage instance, most of the pipelines need to be executed manually since it's losing the reference from [magic/utils/helpers.py](/mageai/magic/utils/helpers.py) on the schedulers that run the python blocks. 

## LLMs used

LLMs _(Large Language Models)_ are a core utility used in this solution. 

Mainly they are used in:
1. Transactions JSON generated from file text
2. To predict a category passing as reference the top 2 results from the knowledge base (VectorDB - Elasticsearch)

We tried to use some models offered freely from providers like [Groq Console](https://console.groq.com/playground) which offers a `llama-3.1-70b-versatile` available, but the results for the transactions list from each file text did not provide a constant `JSON` text to extract.

So we tried another model used with minimum cost like [OpenAI](https://platform.openai.com/)'s Platform page, selecting `gpt-4-0125-preview`.

All implementations of this model are defined in the [helpers.py](/mageai/magic/utils/helpers.py) file.


## UI Images

Home
![home](/docs/app/app_home.png)

Transactions
![transactions](/docs/app/app_transactions.png)

Dashboards
![dashboards](/docs/app/app_dashboards_user.png)

Feedback
![feedback](/docs/app/app_feedback.png)